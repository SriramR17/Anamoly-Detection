# Network Anomaly Detection System - Complete Documentation

## ğŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Data Analysis](#data-analysis)
4. [Machine Learning Pipeline](#machine-learning-pipeline)
5. [Backend API (FastAPI)](#backend-api-fastapi)
6. [Frontend Dashboard (React)](#frontend-dashboard-react)
7. [Installation & Setup](#installation--setup)
8. [Usage Guide](#usage-guide)
9. [API Reference](#api-reference)
10. [Results & Performance](#results--performance)
11. [Directory Structure](#directory-structure)
12. [Development Guide](#development-guide)

---

## ğŸ¯ Project Overview

This is a comprehensive **Network Anomaly Detection System** designed to identify unusual patterns in network traffic data. The system combines machine learning algorithms with a modern web interface to provide real-time monitoring and analysis capabilities.

### Key Features
- **Multi-Algorithm ML Pipeline**: Implements and compares 11+ machine learning algorithms
- **Real-time Dashboard**: Interactive React-based web interface
- **REST API**: FastAPI backend for serving predictions and analytics
- **Data Visualization**: Comprehensive charts and analytics
- **Performance Analysis**: Detailed model comparison and evaluation metrics

### Technologies Used
- **Backend**: Python, FastAPI, scikit-learn, pandas, numpy
- **Frontend**: React, TypeScript, Tailwind CSS, Recharts
- **ML Libraries**: LightGBM, XGBoost, CatBoost, scikit-learn
- **Visualization**: matplotlib, seaborn, React components

---

## ğŸ—ï¸ Architecture

The system follows a modern three-tier architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API   â”‚    â”‚  ML Pipeline    â”‚
â”‚   (React)       â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (Python)      â”‚
â”‚   Port: 5173    â”‚    â”‚   Port: 8000    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚    â”‚   API Endpoints â”‚    â”‚   Trained       â”‚
â”‚   - Visualizationâ”‚    â”‚   - Data servingâ”‚    â”‚   Models        â”‚
â”‚   - Analytics   â”‚    â”‚   - CORS        â”‚    â”‚   - Predictions â”‚
â”‚   - Real-time   â”‚    â”‚   - Statistics  â”‚    â”‚   - Evaluation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. **Raw Data** â†’ CSV files containing network metrics
2. **Preprocessing** â†’ Feature engineering and normalization
3. **Model Training** â†’ Multiple algorithms trained and evaluated
4. **Prediction** â†’ Best model generates anomaly predictions
5. **API Layer** â†’ FastAPI serves data to frontend
6. **Visualization** â†’ React dashboard displays results

---

## ğŸ“Š Data Analysis

### Dataset Description
The system uses ML-MATT Competition data focusing on network cell performance:

#### Training Data (`ML-MATT-CompetitionQT1920_train.csv`)
- **Records**: 36,904 samples
- **Features**: 13 columns
- **Target**: `Unusual` (binary classification)

#### Test Data (`ML-MATT-CompetitionQT1920_test.csv`)
- **Records**: 9,158 samples  
- **Features**: 12 columns (no target)

### Feature Set
```python
NUMERIC_FEATURES = [
    'PRBUsageUL',        # Physical Resource Block Usage - Uplink
    'PRBUsageDL',        # Physical Resource Block Usage - Downlink  
    'meanThr_DL',        # Mean Throughput - Downlink
    'meanThr_UL',        # Mean Throughput - Uplink
    'maxThr_DL',         # Maximum Throughput - Downlink
    'maxThr_UL',         # Maximum Throughput - Uplink
    'meanUE_DL',         # Mean User Equipment - Downlink
    'meanUE_UL',         # Mean User Equipment - Uplink
    'maxUE_DL',          # Maximum User Equipment - Downlink
    'maxUE_UL',          # Maximum User Equipment - Uplink
    'maxUE_UL+DL',       # Total Maximum User Equipment
]

CATEGORICAL_FEATURES = [
    'Time',              # Time of measurement (HH:MM format)
    'CellName',          # Network cell identifier
]
```

### Engineered Features
The pipeline creates additional features:
- `total_users`: Sum of uplink and downlink mean users
- Scaled versions of all numeric features using StandardScaler

### Data Quality
- **Missing Values**: Handled with median imputation
- **Infinite Values**: Replaced with median values
- **Encoding**: Categorical variables processed appropriately
- **Scaling**: StandardScaler applied to numeric features

---

## ğŸ¤– Machine Learning Pipeline

### Algorithm Comparison
The system evaluates 11 different machine learning algorithms:

| Algorithm | Accuracy | F1-Score | Precision | Recall | Training Time |
|-----------|----------|----------|-----------|---------|---------------|
| **LightGBM** | 99.21% | 98.54% | 99.95% | 97.17% | 0.172s |
| **XGBoost** | 98.48% | 97.18% | 99.87% | 94.63% | 0.161s |
| **Random Forest** | 97.99% | 96.23% | 99.68% | 93.02% | 0.411s |
| **CatBoost** | 97.95% | 96.15% | 99.92% | 92.65% | 0.526s |

### Best Model: LightGBM
- **Performance**: Highest accuracy (99.21%) and F1-score (98.54%)
- **Speed**: Fast training time (0.172 seconds)
- **Precision**: Excellent (99.95%) - low false positives
- **Recall**: Strong (97.17%) - good anomaly detection

### Model Training Process
1. **Data Loading**: Load training and test datasets
2. **Preprocessing**: Feature engineering and scaling
3. **Cross-Validation**: 5-fold stratified cross-validation
4. **Model Selection**: Compare multiple algorithms
5. **Model Persistence**: Save best model using joblib
6. **Prediction**: Generate predictions on test set

### Feature Engineering
```python
# Core features used for training
SELECTED_FEATURES = [
    'meanUE_UL',         # Mean uplink users
    'meanUE_DL',         # Mean downlink users  
    'PRBUsageUL',        # Uplink resource usage
    'PRBUsageDL',        # Downlink resource usage
    'total_users'        # Engineered: total user count
]
```

---

## ğŸ”¥ Backend API (FastAPI)

### Architecture Overview
The FastAPI backend serves as the bridge between the ML pipeline and frontend dashboard.

#### Key Components
- **CORS Middleware**: Enables frontend-backend communication
- **Model Loading**: Loads pre-trained models and results
- **Data Processing**: Real-time data analysis and statistics
- **Error Handling**: Comprehensive error management

### Core Functionality

#### 1. Dashboard Data Service
```python
@app.get("/api/dashboard")
async def get_dashboard_data()
```
- Provides summary statistics
- Calculates anomaly rates
- Identifies peak hours
- Returns recent anomalies

#### 2. Model Performance Service  
```python
@app.get("/api/best_models")
async def get_best_models()
```
- Returns top-performing models
- Provides detailed metrics comparison
- Supports model selection interface

#### 3. Predictions Service
```python
@app.get("/api/predictions") 
async def get_predictions()
```
- Serves complete prediction dataset
- Includes confidence scores
- Supports data filtering

#### 4. Analytics Service
```python
@app.get("/api/anomaly_time_series")
async def get_anomaly_time_series()
```
- Time-based anomaly analysis
- Hourly aggregation
- Trend identification

### Data Models
```python
# Dashboard response structure
{
    "status": "success",
    "data": {
        "total_samples": 9158,
        "total_anomalies": 2356, 
        "anomaly_rate": 25.73,
        "peak_hours": 17,
        "peak_rate": 28.9,
        "recent_anomalies": [...],
        "last_updated": "Just now"
    }
}
```

---

## ğŸ’» Frontend Dashboard (React)

### Architecture Overview
Modern React application built with TypeScript and Tailwind CSS.

#### Key Technologies
- **React 18**: Latest React features and hooks
- **TypeScript**: Type-safe development
- **Tailwind CSS**: Utility-first styling
- **Recharts**: Data visualization library
- **Lucide React**: Modern icon library
- **Axios**: HTTP client for API calls

### Component Structure

#### 1. Main App Component (`App.tsx`)
- **State Management**: Manages global application state
- **API Integration**: Handles all backend communication
- **Tab Navigation**: Dashboard, Predictions, Statistics views
- **Real-time Updates**: Automatic data refresh

#### 2. Dashboard View Components

##### StatsCard Component
```typescript
interface StatsCardProps {
    title: string;
    value: string | number;
    icon: IconType;
    color: 'blue' | 'red' | 'green' | 'orange' | 'purple';
}
```
- Displays key metrics
- Animated hover effects
- Color-coded indicators

##### EvaluationMetrics Component
```typescript
interface MetricItem {
    name: string;
    value: number;
    color: string;
    icon: IconType;
}
```
- Gauge charts for model performance
- Real-time metric display
- Visual performance indicators

##### AlgorithmComparison Component
```typescript
interface AlgoPerf {
    name: string;
    accuracy: number;
    f1: number;
    precision: number;
    recall: number;
}
```
- Interactive bar charts
- Model selection interface
- Detailed performance comparison

##### PeakHoursClock Component
- Animated clock visualization
- Dynamic peak hour identification
- Real-time updates

##### TimeByAnomalyGraph Component
- Line chart for time series analysis
- 24-hour anomaly patterns
- Interactive tooltips

#### 3. Predictions View (`PredictionsTable.tsx`)
- **Data Filtering**: Filter by anomalies/normal traffic
- **Search Functionality**: Search by cell name or time
- **Pagination**: Handle large datasets efficiently
- **Detail Modal**: Comprehensive anomaly analysis
- **Grouping Options**: Cell-wise or time-based grouping

#### 4. Statistics View (`Statistics.tsx`)
- **Pie Charts**: Data distribution visualization
- **Bar Charts**: Categorical analysis
- **Probability Statistics**: Confidence metrics
- **Performance Insights**: Automated analysis

### API Integration (`api.ts`)
```typescript
const API_BASE_URL = 'http://localhost:8000';

export const apiService = {
    getDashboard: () => Promise<ApiResponse<DashboardData>>,
    getBestModels: () => Promise<ApiResponse<BestModelData[]>>,
    getPredictions: () => Promise<ApiResponse<Prediction[]>>,
    getStats: () => Promise<ApiResponse<Statistics>>,
    getAnomalyTimeSeries: () => Promise<ApiResponse<AnomalyTimePoint[]>>
};
```

### Responsive Design
- **Mobile-First**: Optimized for all screen sizes
- **Grid Layouts**: CSS Grid and Flexbox
- **Adaptive Components**: Dynamic component sizing
- **Touch-Friendly**: Mobile interaction support

---

## ğŸš€ Installation & Setup

### Prerequisites
```bash
# System Requirements
- Python 3.8+ 
- Node.js 16+
- npm or yarn
- Git
```

### Backend Setup

#### 1. Clone Repository
```bash
git clone <repository-url>
cd Anamoly-Detection
```

#### 2. Create Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 3. Install Python Dependencies
```bash
pip install -r requirements.txt
```

#### 4. Install API Dependencies
```bash
cd api
pip install -r requirements.txt
```

#### 5. Run ML Pipeline (Optional - Results Included)
```bash
# From project root
python main.py
```

#### 6. Start Backend Server
```bash
cd api
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

### Frontend Setup

#### 1. Install Dependencies
```bash
cd frontend
npm install
```

#### 2. Start Development Server
```bash
npm run dev
```

### Verification
- **Backend**: http://localhost:8000
- **Frontend**: http://localhost:5173
- **API Docs**: http://localhost:8000/docs

---

## ğŸ“– Usage Guide

### 1. Running the Complete System

#### Start Backend
```bash
cd api
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

#### Start Frontend
```bash
cd frontend  
npm run dev
```

### 2. Dashboard Navigation

#### Main Dashboard
- **Overview Cards**: Total samples, anomalies, detection rate
- **Performance Metrics**: Model accuracy, F1-score, precision, recall
- **Algorithm Comparison**: Interactive model performance charts
- **Peak Hours Clock**: Visual peak anomaly time identification
- **Distribution Charts**: Anomaly vs normal traffic breakdown

#### Predictions View
- **View Modes**: Switch between cell-wise and time-based analysis
- **Filtering**: Show all, anomalies only, or normal traffic only
- **Search**: Find specific cells or time periods
- **Sorting**: Order by anomaly rate (high to low, low to high)
- **Pagination**: Navigate through large datasets
- **Detail Modal**: Click any prediction for comprehensive analysis

#### Statistics View
- **Summary Cards**: Key statistical measures
- **Distribution Charts**: Pie and bar charts
- **Probability Analysis**: Confidence score distributions
- **Trend Analysis**: Time-based patterns

### 3. API Usage

#### Get Dashboard Data
```bash
curl http://localhost:8000/api/dashboard
```

#### Get All Predictions
```bash
curl http://localhost:8000/api/predictions
```

#### Get Model Comparison
```bash
curl http://localhost:8000/api/best_models
```

### 4. Running ML Pipeline

#### Full Pipeline Execution
```bash
python main.py
```

#### Individual Components
```bash
cd src
python data_loader.py      # Test data loading
python preprocessor.py     # Test preprocessing  
python model_trainer.py    # Train models
python pipeline.py         # Run complete pipeline
```

---

## ğŸ”Œ API Reference

### Base URL
```
http://localhost:8000
```

### Endpoints

#### 1. Health Check
```http
GET /
```
**Response:**
```json
{
    "message": "Anomaly Detection API is running!"
}
```

#### 2. Dashboard Data
```http
GET /api/dashboard
```
**Response:**
```json
{
    "status": "success",
    "data": {
        "total_samples": 9158,
        "total_anomalies": 2356,
        "anomaly_rate": 25.73,
        "peak_hours": 17,
        "peak_rate": 28.9,
        "recent_anomalies": [...],
        "last_updated": "Just now"
    }
}
```

#### 3. Model Performance
```http
GET /api/best_models  
```
**Response:**
```json
{
    "status": "success",
    "data": [
        {
            "LightGBM": {
                "Accuracy_Mean": 0.9920,
                "F1_Mean": 0.9854,
                "Precision_Mean": 0.9995,
                "Recall_Mean": 0.9717
            }
        }
    ]
}
```

#### 4. Evaluation Metrics
```http
GET /api/get_evaluation_metrics
```
**Response:**
```json
{
    "status": "success", 
    "data": {
        "Accuracy": 0.9920,
        "F1_Score": 0.9854,
        "Precision": 0.9995,
        "Recall": 0.9717
    }
}
```

#### 5. All Predictions
```http
GET /api/predictions
```
**Response:**
```json
{
    "status": "success",
    "data": [
        {
            "Time": "3:00",
            "CellName": "6ALTE", 
            "PRBUsageUL": 3.781,
            "PRBUsageDL": 1.493,
            "Predicted_Anomaly": 1,
            "Anomaly_Probability": 0.9424
        }
    ],
    "total": 9158
}
```

#### 6. Statistics
```http
GET /api/stats
```
**Response:**
```json
{
    "status": "success",
    "data": {
        "total_records": 9158,
        "anomalies": 2356,
        "normal": 6802,
        "avg_anomaly_prob": 0.2573,
        "max_anomaly_prob": 0.9998,
        "min_anomaly_prob": 0.0001
    }
}
```

#### 7. Time Series Data
```http
GET /api/anomaly_time_series
```
**Response:**
```json
{
    "status": "success",
    "data": [
        {"hour": 0, "anomalies": 45},
        {"hour": 1, "anomalies": 52},
        {"hour": 2, "anomalies": 38}
    ]
}
```

### Error Responses
```json
{
    "detail": "Error message description"
}
```

### CORS Configuration
The API supports CORS for frontend integration:
```python
allow_origins=["http://localhost:5173"]
allow_methods=["*"] 
allow_headers=["*"]
```

---

## ğŸ“ˆ Results & Performance

### Model Performance Summary

| Metric | LightGBM | XGBoost | Random Forest | CatBoost |
|--------|----------|---------|---------------|----------|
| **Accuracy** | 99.21% | 98.48% | 97.99% | 97.95% |
| **F1-Score** | 98.54% | 97.18% | 96.23% | 96.15% |
| **Precision** | 99.95% | 99.87% | 99.68% | 99.92% |
| **Recall** | 97.17% | 94.63% | 93.02% | 92.65% |
| **Training Time** | 0.172s | 0.161s | 0.411s | 0.526s |

### Anomaly Detection Results

#### Dataset Statistics
- **Total Test Samples**: 9,158
- **Detected Anomalies**: 2,356 (25.73%)
- **Normal Traffic**: 6,802 (74.27%)

#### Temporal Patterns
- **Peak Hour**: 17:00 (5:00 PM)
- **Peak Anomaly Rate**: 28.9% at peak hour
- **Lowest Activity**: Early morning hours (3:00-5:00 AM)

#### Confidence Distribution
- **Average Confidence**: 25.73%
- **Maximum Confidence**: 99.98%
- **High Confidence Anomalies**: 847 samples (>90% confidence)

### Performance Insights

#### Model Selection Rationale
**LightGBM** was selected as the best model because:
1. **Highest Accuracy**: 99.21% overall classification accuracy
2. **Balanced Performance**: Strong precision (99.95%) and recall (97.17%)
3. **Fast Training**: Quick iteration and deployment capability
4. **Low False Positives**: Critical for anomaly detection systems

#### Real-World Impact
- **False Positive Rate**: 0.05% (Precision: 99.95%)
- **False Negative Rate**: 2.83% (Recall: 97.17%)
- **System Reliability**: 99.21% correct classifications

### Visual Results
The system generates several visualizations:
- `results/exploration.png` - Data exploration charts
- `results/algorithm_comparison.png` - Model performance comparison
- Interactive dashboard charts for real-time monitoring

---

## ğŸ“ Directory Structure

```
Anamoly-Detection/
â”œâ”€â”€ ğŸ“„ main.py                     # Main entry point
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“„ PROJECT_DOCUMENTATION.md    # This documentation
â”œâ”€â”€ ğŸ“„ .gitignore                  # Git ignore rules
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“‚ api/                        # FastAPI Backend
â”‚   â”œâ”€â”€ ğŸ“„ app.py                  # FastAPI application
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt        # API dependencies
â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”œâ”€â”€ ğŸ“‚ frontend/                   # React Frontend  
â”‚   â”œâ”€â”€ ğŸ“„ package.json            # Node.js dependencies
â”‚   â”œâ”€â”€ ğŸ“„ vite.config.ts          # Vite configuration
â”‚   â”œâ”€â”€ ğŸ“„ tailwind.config.js      # Tailwind CSS config
â”‚   â”œâ”€â”€ ğŸ“„ index.html              # HTML entry point
â”‚   â”œâ”€â”€ ğŸ“‚ src/                    # React source code
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ App.tsx             # Main application component
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ main.tsx            # React entry point
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ api.ts              # API service layer
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ types.d.ts          # TypeScript definitions
â”‚   â”‚   â””â”€â”€ ğŸ“‚ components/         # React components
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ PredictionsTable.tsx  # Predictions view
â”‚   â”‚       â””â”€â”€ ğŸ“„ Statistics.tsx        # Statistics view
â”‚   â””â”€â”€ ğŸ“‚ dist/                   # Built frontend assets
â”‚
â”œâ”€â”€ ğŸ“‚ src/                        # ML Pipeline Source
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ config.py               # Configuration settings
â”‚   â”œâ”€â”€ ğŸ“„ data_loader.py          # Data loading utilities
â”‚   â”œâ”€â”€ ğŸ“„ preprocessor.py         # Data preprocessing
â”‚   â”œâ”€â”€ ğŸ“„ explorer.py             # Data exploration
â”‚   â”œâ”€â”€ ğŸ“„ model_trainer.py        # ML model training
â”‚   â””â”€â”€ ğŸ“„ pipeline.py             # Complete ML pipeline
â”‚
â”œâ”€â”€ ğŸ“‚ data/                       # Datasets
â”‚   â”œâ”€â”€ ğŸ“„ ML-MATT-CompetitionQT1920_train.csv  # Training data (36,904 samples)
â”‚   â””â”€â”€ ğŸ“„ ML-MATT-CompetitionQT1920_test.csv   # Test data (9,158 samples)
â”‚
â”œâ”€â”€ ğŸ“‚ models/                     # Trained Models
â”‚   â”œâ”€â”€ ğŸ“„ best_model.joblib       # Best performing model (LightGBM)
â”‚   â”œâ”€â”€ ğŸ“„ feature_scaler.joblib   # Feature scaling transformer
â”‚   â””â”€â”€ ğŸ“„ cellname_encoder.joblib # Cell name encoder
â”‚
â”œâ”€â”€ ğŸ“‚ results/                    # Analysis Results
â”‚   â”œâ”€â”€ ğŸ“„ predictions.csv         # Complete predictions (9,158 rows)
â”‚   â”œâ”€â”€ ğŸ“„ simple_predictions.csv  # Competition format
â”‚   â”œâ”€â”€ ğŸ“„ algorithm_comparison_results.csv  # Model comparison
â”‚   â”œâ”€â”€ ğŸ“„ exploration.png         # Data visualization
â”‚   â””â”€â”€ ğŸ“„ algorithm_comparison.png # Performance charts
â”‚
â”œâ”€â”€ ğŸ“‚ architecture/               # System Architecture
â”‚   â””â”€â”€ ğŸ“„ architecture_diagram.png # System architecture diagram
â”‚
â””â”€â”€ ğŸ“‚ tests/                      # Test Files  
    â””â”€â”€ ğŸ“„ test_outputs.py         # Output validation tests
```

### File Descriptions

#### Core Files
- `main.py` - Entry point for running the complete ML pipeline
- `requirements.txt` - All Python dependencies for the ML pipeline

#### Backend (`api/`)
- `app.py` - FastAPI application with all endpoints and business logic
- `requirements.txt` - Specific API dependencies (FastAPI, uvicorn, etc.)

#### Frontend (`frontend/`)
- `src/App.tsx` - Main React component with dashboard, routing, and state management
- `src/api.ts` - Axios-based API client for backend communication
- `src/components/` - Reusable React components for different views

#### ML Pipeline (`src/`)
- `config.py` - Central configuration (file paths, model parameters, feature definitions)
- `data_loader.py` - CSV loading and basic validation
- `preprocessor.py` - Feature engineering, scaling, and data cleaning
- `model_trainer.py` - Model training, evaluation, and comparison logic
- `pipeline.py` - Orchestrates the complete ML workflow

#### Data & Results
- `data/` - Original datasets from ML-MATT competition
- `models/` - Serialized trained models and preprocessing artifacts  
- `results/` - Generated predictions, analysis results, and visualizations

---

## ğŸ”§ Development Guide

### Adding New Features

#### 1. Adding New API Endpoints

**Backend** (`api/app.py`):
```python
@app.get("/api/new_endpoint")
async def new_endpoint():
    try:
        # Your logic here
        return {"status": "success", "data": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Frontend** (`src/api.ts`):
```typescript
export const apiService = {
    // Existing methods...
    getNewData: async (): Promise<ApiResponse<NewDataType>> => {
        const response = await api.get<ApiResponse<NewDataType>>('/api/new_endpoint');
        return response.data;
    }
};
```

#### 2. Adding New ML Models

**Configuration** (`src/config.py`):
```python
MODEL_PARAMS = {
    'NewModel': {'param1': value1, 'param2': value2},
    # Existing models...
}
```

**Model Training** (`src/model_trainer.py`):
```python
def get_models():
    models = {
        'NewModel': NewModelClass(**MODEL_PARAMS['NewModel']),
        # Existing models...
    }
    return models
```

#### 3. Adding New Dashboard Components

**Create Component** (`src/components/NewComponent.tsx`):
```typescript
import React from 'react';

interface NewComponentProps {
    data: DataType;
}

const NewComponent: React.FC<NewComponentProps> = ({ data }) => {
    return (
        <div className="bg-gray-800 rounded-xl p-6 border border-gray-700">
            {/* Component JSX */}
        </div>
    );
};

export default NewComponent;
```

**Integrate in App** (`src/App.tsx`):
```typescript
import NewComponent from './components/NewComponent';

// In render method:
<NewComponent data={newData} />
```

### Code Style Guidelines

#### Python (Backend & ML)
```python
# Use type hints
def process_data(data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """
    Process data with comprehensive docstring.
    
    Args:
        data: Input DataFrame
        
    Returns:
        Tuple of processed arrays
    """
    pass

# Use descriptive variable names
anomaly_probability = model.predict_proba(features)[:, 1]

# Follow PEP 8 formatting
class AnomalyDetector:
    def __init__(self, model_type: str = 'lightgbm'):
        self.model_type = model_type
```

#### TypeScript (Frontend)
```typescript
// Use interfaces for type safety
interface AnomalyData {
    timestamp: string;
    cellName: string;
    probability: number;
    isAnomaly: boolean;
}

// Use descriptive function names
const fetchAnomalyData = async (): Promise<AnomalyData[]> => {
    // Implementation
};

// Use React best practices
const AnomalyCard: React.FC<{ anomaly: AnomalyData }> = ({ anomaly }) => {
    const [isExpanded, setIsExpanded] = useState(false);
    
    useEffect(() => {
        // Cleanup on unmount
        return () => cleanup();
    }, []);
};
```

### Testing

#### Backend Testing
```bash
# Test individual components
cd src
python data_loader.py
python preprocessor.py
python model_trainer.py

# Test API endpoints
curl http://localhost:8000/api/dashboard
```

#### Frontend Testing  
```bash
# Development server
npm run dev

# Build production version
npm run build

# Lint code
npm run lint
```

### Performance Optimization

#### Backend Optimizations
1. **Caching**: Implement Redis for frequently accessed data
2. **Database**: Use PostgreSQL for large-scale deployment
3. **Async Processing**: Use Celery for background tasks
4. **Model Loading**: Cache models in memory

#### Frontend Optimizations
1. **Code Splitting**: Implement React.lazy for route-based splitting
2. **Memoization**: Use React.memo for expensive components
3. **Virtual Scrolling**: For large data tables
4. **API Optimization**: Implement data pagination and filtering

### Deployment Considerations

#### Production Environment
```bash
# Backend
gunicorn app:app -w 4 -k uvicorn.workers.UvicornWorker

# Frontend
npm run build
# Serve with nginx or similar
```

#### Docker Deployment
```dockerfile
# Backend Dockerfile
FROM python:3.9-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

# Frontend Dockerfile  
FROM node:16-alpine
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build
```

#### Environment Variables
```bash
# Backend (.env)
DATABASE_URL=postgresql://...
REDIS_URL=redis://...
MODEL_PATH=/app/models/

# Frontend (.env)
VITE_API_BASE_URL=http://api.yourdomain.com
VITE_ENVIRONMENT=production
```

---

## ğŸ‰ Conclusion

This Network Anomaly Detection System demonstrates a complete end-to-end machine learning application with:

âœ… **Robust ML Pipeline**: 99.21% accuracy with LightGBM  
âœ… **Modern Web Interface**: React + TypeScript dashboard  
âœ… **Production-Ready API**: FastAPI with comprehensive endpoints  
âœ… **Real-Time Monitoring**: Live anomaly detection and visualization  
âœ… **Scalable Architecture**: Modular design for easy extension  

The system successfully processes 46,000+ network samples, detects anomalies with high precision, and provides intuitive visualizations for network administrators and security teams.

### Key Achievements
- **High Performance**: 99.21% accuracy, 98.54% F1-score
- **Fast Processing**: < 0.2s training time per model
- **Comprehensive Analysis**: 11 algorithm comparison
- **User Experience**: Interactive, responsive dashboard
- **Production Ready**: Complete documentation and deployment guide

### Next Steps
1. **Scalability**: Implement real-time data streaming
2. **Advanced ML**: Add deep learning models (LSTM, Transformer)
3. **Monitoring**: Add system health monitoring and alerting
4. **Security**: Implement authentication and authorization
5. **Analytics**: Add advanced statistical analysis and reporting

---

**Last Updated**: September 1, 2025  
**Version**: 1.0.0  
**Author**: Anomaly Detection System Team
