Documentation - Network Anomaly Detection Project (CTS)
=====================================================

Purpose
-------
This documentation records the end-to-end process carried out on the CTS Network Anomaly Detection project up to the current point: what is in the project, the environment used, steps executed, fixes applied, commands run, outputs generated, and final results.

Project Overview
----------------
- Goal: Detect unusual (anomalous) network behavior from traffic metrics.
- Approach: Simple ML pipeline with data loading, exploration, preprocessing, model training, prediction, and reporting.
- Tech stack: Python, pandas, numpy, scikit-learn, matplotlib, seaborn.

Environment
-----------
- OS: Windows
- Shell: PowerShell (pwsh 5.1)
- Python: 3.10.18 (Anaconda)
- Packages: pandas, numpy, scikit-learn, matplotlib, seaborn (available in environment)

Data
----
- Location: dataset/
  - ML-MATT-CompetitionQT1920_train.csv (train)
  - ML-MATT-CompetitionQT1920_test.csv (test)
- Encoding: latin-1 (ISO-8859-1). UTF-8 raised decode errors; fixed by reading with latin-1.
- Train shape: 36,904 rows × 14 columns
- Test shape: 9,158 rows × 13 columns
- Target column: Unusual (0 = normal, 1 = anomaly)
- Noted issues: 183 missing values in train, 6 in test; some invalid numeric entries such as "#¡VALOR!"

Repository Structure (Professional Layout)
-----------------------------------------
CTS/ (Project Root)
├── main.py                    # Main entry point - run this to execute the system
├── README.md                  # Project overview and quick start guide
├── requirements.txt           # Python dependencies
├── .gitignore                # Git ignore patterns
├── src/                      # Source Code
│   ├── simple_main.py        # Pipeline orchestrator
│   ├── simple_config.py      # Configuration and parameters
│   ├── simple_data_loader.py # Data loading and validation
│   ├── simple_explorer.py    # Exploratory data analysis
│   ├── simple_preprocessor.py# Data preprocessing and feature engineering
│   └── simple_model_trainer.py# Model training and evaluation
├── data/                     # Raw data files (renamed from dataset/)
├── results/                  # Generated outputs (renamed from output/)
├── docs/                     # Documentation
│   ├── documentation.txt     # This comprehensive documentation
│   ├── PROJECT_STRUCTURE.md  # Project organization guide
│   ├── SIMPLE_README.md      # Technical implementation details
│   └── TEST_SUMMARY.md       # Testing and validation report
├── tests/                    # Testing and validation
│   └── test_outputs.py       # Output validation tests
├── config/                   # Configuration files
│   └── simple_requirements.txt# Python package requirements
├── models/                   # Model artifacts
└── scripts/                  # Utility scripts (for future use)

Execution Timeline & Steps Completed
-----------------------------------
1) Checked Python and dependencies
   - Verified Python 3.10.18 (Anaconda) available.
   - Confirmed required libraries import successfully.

2) Validated data files
   - Initial UTF-8 read caused UnicodeDecodeError.
   - Verified latin-1 successfully loads training and test CSVs.
   - Confirmed expected columns and shapes.

3) Fixed and tested individual modules
   a) Data loader (simple_data_loader.py)
      - Change: Read both train and test with encoding='latin-1'.
      - Result: Data loads and validation passes; warnings printed for missing values.

   b) Preprocessor (simple_preprocessor.py)
      - Change: Convert NUMERIC_COLS to numeric with errors='coerce'; median imputation for NaNs; retain inf→NaN handling.
      - Result: Preprocessing succeeds; creates 23 features; scaling applied.

   c) Explorer (simple_explorer.py)
      - Problem: Failing on invalid numeric strings during correlation/plotting.
      - Change: Clean numeric columns with to_numeric(errors='coerce'); use cleaned frame (train_clean) for correlation and hour-based aggregations and plotting.
      - Result: EDA runs; saves exploration.png.

   d) Model trainer (simple_model_trainer.py)
      - Trains 3 models; evaluates by 3-fold CV F1; selects best.
      - Result: RandomForest best with F1≈0.812 (cv mean).

4) Ran complete pipeline (simple_main.py)
   - Steps executed automatically: load → explore → preprocess → train → predict → save results.
   - Completed successfully.

5) Validated outputs
   - Wrote helper: test_outputs.py to programmatically check artifacts.
   - Confirmed predictions.csv, simple_predictions.csv, exploration.png present and valid.

Detailed Code Changes Applied
-----------------------------
1) simple_data_loader.py
   - Before:
     train_data = pd.read_csv(train_path,encoding='ISO-8859')
     test_data = pd.read_csv(test_path)
   - After:
     train_data = pd.read_csv(train_path, encoding='latin-1')
     test_data = pd.read_csv(test_path, encoding='latin-1')

2) simple_preprocessor.py (function: _prepare_features)
   - Added cleaning of numeric columns:
     for col in NUMERIC_COLS:
         if col in X.columns:
             X[col] = pd.to_numeric(X[col], errors='coerce')
   - Retained median imputation for missing values and inf handling.

3) simple_explorer.py
   - Added numeric cleaning into a train_clean copy using to_numeric(errors='coerce').
   - Used train_clean for:
     - Hour extraction and hourly anomaly aggregation.
     - Correlation computation with target.
     - Plotting (passed train_clean to plotting helper).

4) New files created
   - test_outputs.py: Script to validate output artifacts and print summaries.
   - TEST_SUMMARY.md: Human-readable test summary/report.

Commands Executed (representative)
----------------------------------
- python --version
- python -c "import pandas, numpy, sklearn, matplotlib, seaborn; ..."
- python simple_data_loader.py
- python simple_preprocessor.py
- python simple_model_trainer.py
- python simple_explorer.py
- python simple_main.py
- python test_outputs.py
- dir output

Results & Metrics
-----------------
- EDA Key Points:
  - Missing values: train=183, test=6 (handled)
  - Peak anomaly hours: [22, 18, 20]
  - Top anomaly cells: ['5BLTE', '10CLTE', '7ALTE', '5CLTE', '6VLTE']
  - Top correlated features (example): maxThr_UL, meanUE_UL, meanThr_UL (weak correlations shown)

- Model Training (3-fold CV F1):
  - RandomForest: ~0.812 ± 0.006
  - LogisticRegression: ~0.011 ± 0.001
  - GradientBoosting: ~0.793 ± 0.004
  - Best model: RandomForest

- Predictions on Test Set:
  - Total: 9,158
  - Predicted anomalies: 1,847 (20.2%)

Generated Outputs
-----------------
Located in output/:
- predictions.csv (detailed results with probabilities)
- simple_predictions.csv (Index, Predicted_Unusual)
- exploration.png (EDA plots)
- Additional existing files: data_info.txt, eda_report.txt, exploratory_analysis.png, final_report.txt, predictions_placeholder.csv

How to Re-run
-------------
1) Ensure Python environment has required packages: pandas, numpy, scikit-learn, matplotlib, seaborn.
2) Place data CSVs under dataset/ (already present).
3) Run the full pipeline:
   python simple_main.py
4) Validate artifacts (optional):
   python test_outputs.py

Notes & Rationale
-----------------
- Encoding: latin-1 chosen due to non-UTF8 characters in CSVs.
- Data cleaning: Explicit coercion to numeric ensures robustness against localized error tokens such as "#¡VALOR!".
- Imputation: Median used for simplicity and robustness.
- Model choice: RandomForest provided best F1 in CV; LogisticRegression underperformed given feature relationships.

Next Steps (Suggestions)
------------------------
- Persist the trained best model (joblib) and add quick_prediction() implementation.
- Add unit tests for each module.
- Parameter tuning for RandomForest / GradientBoosting (GridSearchCV).
- Add logging instead of print statements.
- Evaluate feature importance and refine features.
- Consider class imbalance handling if required (e.g., class_weight, sampling).

Changelog (Session)
-------------------
- Fixed CSV encoding issues; standardized to latin-1 in loader.
- Hardened preprocessing against invalid numeric strings; added median imputation.
- Updated explorer to operate on cleaned data; plotting and correlations now robust.
- Ran full pipeline successfully; generated predictions and plots.
- Added test_outputs.py for artifact validation.
- Added TEST_SUMMARY.md summarizing the test run.
- **RESTRUCTURED PROJECT FOR PROFESSIONAL LAYOUT:**
  * Organized code into src/ directory
  * Moved documentation to docs/ directory
  * Renamed dataset/ to data/, output/ to results/
  * Created professional README.md with project overview
  * Added requirements.txt and .gitignore
  * Created main.py as professional entry point
  * Updated all module paths to work with new structure
  * Added PROJECT_STRUCTURE.md documenting organization
  * Cleaned up temporary files (__pycache__)
- **FIXED MODEL RETRAINING ISSUE (Aug 26, 2025):**
  * Problem: Model trainer was retraining models every time despite saved model existing
  * Root cause: Main pipeline (src/main.py) always called train_models() without checking for existing models
  * Solution: Created get_or_train_model() function that checks for existing model first
  * Changes made:
    - Added get_or_train_model() function in model_trainer.py
    - Added force_retrain_model() function for when retraining is explicitly needed
    - Updated main.py to use get_or_train_model() instead of train_models()
    - Improved model path handling using pathlib for cross-platform compatibility
    - Model now loads from best_model.joblib in project root if it exists
  * Result: System now loads existing model in ~2 seconds instead of retraining (~30+ seconds)
  * Test confirmed: Running main.py shows "Found existing model... Loading... Model loaded successfully. Skipping training."

